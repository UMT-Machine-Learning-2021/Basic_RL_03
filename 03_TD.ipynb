{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Temporal Difference Learning\n",
    "---\n",
    "Now that we have learned a little about Monte Carlo Methods, let's move on to Temporal Difference(TD) Learning.\n",
    "The plan for the week is to do a small lecture Tuesday to review the TD material then jump into an exercise.\n",
    "If we complete that exercise, we will do a really fun exercise using the same TD methods Thursday.\n",
    "\n",
    "Please watch the videos bellow.\n",
    "There is not a lot of material here, but it is dense.\n",
    "I recommend taking notes and reviewing the videos a few times until you have a solid understanding of the material.\n",
    "Lastly, don't be afraid to review the MC control material; it is very helpful in understanding the motivation for TD learning.\n",
    "\n",
    "[Introduction](https://www.youtube.com/watch?v=yXErXQulI_o&t=81s)\n",
    "\n",
    "[TD Control Part 1](https://www.youtube.com/watch?v=HYV0SP9wm7g)\n",
    "\n",
    "[TD Control Part 2](https://www.youtube.com/watch?v=U_CV-UC9G2c)\n",
    "\n",
    "[SARSA max (Q-Learning)](https://www.youtube.com/watch?v=4DxoYuR7aZ4)\n",
    "\n",
    "[Expected SARSA](https://www.youtube.com/watch?v=kEKupCyU0P0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise\n",
    "---\n",
    "\n",
    "Now that we know a little about TD methods, let's put them into practice.\n",
    "\n",
    "Run this first cell to get all the helper functions loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "V_opt = np.zeros((4,12))\n",
    "V_opt[0:13][0] = -np.arange(3, 15)[::-1]\n",
    "V_opt[0:13][1] = -np.arange(3, 15)[::-1] + 1\n",
    "V_opt[0:13][2] = -np.arange(3, 15)[::-1] + 2\n",
    "V_opt[3][0] = -13\n",
    "\n",
    "pol_opt = np.hstack((np.ones(11), 2, 0))\n",
    "\n",
    "V_true = np.zeros((4,12))\n",
    "for i in range(3):\n",
    "    V_true[0:13][i] = -np.arange(3, 15)[::-1] - i\n",
    "V_true[1][11] = -2\n",
    "V_true[2][11] = -1\n",
    "V_true[3][0] = -17\n",
    "\n",
    "def get_long_path(V):\n",
    "    return np.array(np.hstack((V[0:13][0], V[1][0], V[1][11], V[2][0], V[2][11], V[3][0], V[3][11])))\n",
    "\n",
    "def get_optimal_path(policy):\n",
    "    return np.array(np.hstack((policy[2][:], policy[3][0])))\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "\n",
    "    def td_prediction_check(self, V):\n",
    "        to_check = get_long_path(V)\n",
    "        soln = get_long_path(V_true)\n",
    "        np.testing.assert_array_almost_equal(soln, to_check)\n",
    "\n",
    "    def td_control_check(self, policy):\n",
    "        to_check = get_optimal_path(policy)\n",
    "        np.testing.assert_equal(pol_opt, to_check)\n",
    "\n",
    "check = Tests()\n",
    "\n",
    "def run_check(check_name, func):\n",
    "    try:\n",
    "        getattr(check, check_name)(func)\n",
    "    except check.failureException as e:\n",
    "        printmd('**<span style=\"color: red;\">PLEASE TRY AGAIN</span>**')\n",
    "        return\n",
    "    printmd('**<span style=\"color: green;\">PASSED</span>**')\n",
    "\n",
    "def plot_values(V):\n",
    "\t# reshape the state-value function\n",
    "\tV = np.reshape(V, (4,12))\n",
    "\t# plot the state-value function\n",
    "\tfig = plt.figure(figsize=(15,5))\n",
    "\tax = fig.add_subplot(111)\n",
    "\tim = ax.imshow(V, cmap='cool')\n",
    "\tfor (j,i),label in np.ndenumerate(V):\n",
    "\t    ax.text(i, j, np.round(label,3), ha='center', va='center', fontsize=14)\n",
    "\tplt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "\tplt.title('State-Value Function')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next let's take a look at the environment.\n",
    "For this exercise we will use the cliff walking environment.\n",
    "This is an example of a grid world.\n",
    "If you want to take a look at the source code, check here [CliffWalking](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves through a $4\\times 12$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    " [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    " [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    " [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "```\n",
    "At the start of any episode, state `36` is the initial state.  State `47` is the only terminal state, and the cliff corresponds to states `37` through `46`.\n",
    "\n",
    "The agent has 4 potential actions:\n",
    "```\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+=\\{0, 1, \\ldots, 47\\}$, and $\\mathcal{A} =\\{0, 1, 2, 3\\}$.\n",
    "\n",
    "As with the previous gym environment, we can examine this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of today's exercise is to find the value function and therefore the policy for the environment.\n",
    "As a quick reminder, the optimal policy is a function that maps from a given state to the best action.\n",
    "Grid worlds are a great place to start because visualizing the optimal state-value function is easy.\n",
    "In the figure below, we visualize the optimal state-value function for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the optimal state-value function\n",
    "V_opt = np.zeros((4,12))\n",
    "V_opt[0:13][0] = -np.arange(3, 15)[::-1]\n",
    "V_opt[0:13][1] = -np.arange(3, 15)[::-1] + 1\n",
    "V_opt[0:13][2] = -np.arange(3, 15)[::-1] + 2\n",
    "V_opt[3][0] = -13\n",
    "\n",
    "plot_values(V_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sarsa\n",
    "\n",
    "The first algorithm we will implement is SARSA.\n",
    "It is a fairly simple algorithm but has its tricky points.\n",
    "\n",
    "The defining trait of SARSA is that all actions are chosen with the epsilon greedy policy, and the value of all state-value pairs are estimated with the epsilon greedy policy.\n",
    "Read the pseudocode carefully, and you should be just fine.\n",
    "\n",
    "\"Well, maybe...\"\n",
    "\n",
    "\\-Less Claypool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Pseudocode\n",
    "---\n",
    "\n",
    "initialize epsilon\n",
    "\n",
    "for episode in num_episodes:\n",
    "\n",
    "    decay epsilon\n",
    "    state <- initial state\n",
    "    chose action from Q with epsilon greedy policy\n",
    "    t <- 0\n",
    "    repeat until terminal state:\n",
    "        take action A_t and observe R_t+1, S_t+1\n",
    "        chose action A_t+1 from Q with epsilon greedy policy\n",
    "        update Q-table\n",
    "        S <- S_t+1\n",
    "        A <- A_t+1\n",
    "---\n",
    "\n",
    "Recall that the Q-table update for SARSA is:\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    \"\"\"\n",
    "    SARSA Algorithm\n",
    "    :param env: open ai gym environment\n",
    "    :param num_episodes: number of episodes to run\n",
    "    :param alpha: weight for the Q-tabel running average\n",
    "    :param gamma: discount factor for future rewards\n",
    "    :return: (dict) Q-table and (list) score for each episode\n",
    "    \"\"\"\n",
    "    # useful for epsilon greedy action selection...\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "\n",
    "    # initialize performance monitor\n",
    "    scores = []\n",
    "\n",
    "    # epsilon decay parameters\n",
    "    eps = 1\n",
    "    eps_decay = 0.9\n",
    "    eps_end = 0.000001\n",
    "\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # decay epsilon\n",
    "        eps = max(eps * eps_decay, eps_end)\n",
    "\n",
    "        ## TODO: complete the function by implementing the remaining pseudocode\n",
    "        ## HINT: env.reset() returns the initial state\n",
    "        ## HINT: env.step() takes an int (action) and returns a tuple of (next_state, reward, done, info)\n",
    "\n",
    "    return Q, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PASSED** indicates that your Q-table is sufficiently close to the true value, well done!\n",
    "**PLEASE TRY AGAIN** indicates that you have a little more work to do.\n",
    "\n",
    "**IMPORTANT** For the tests to pass you can change num_episodes and alpha, but you must leave gamma in the default value.\n",
    "Once the tests pass, and your function is working correctly, then feel free to alter gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa, scores_sarsa = sarsa(env, num_episodes=10000, alpha=0.01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "run_check('td_control_check', policy_sarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "plot_values(V_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SARSA Max a.k.a Q-learning\n",
    "\n",
    "SARSA Max is very similar to SARSA.\n",
    "The main difference comes in the update step for the Q-table.\n",
    "SARSA Max estimates the value of the next state by taking the max value over all action in that state.\n",
    "This is in contrast to SARSA, which estimated the value of the next state by choosing an action with an epsilon greedy policy and using that state-action pair to estimate the value.\n",
    "\n",
    "Put simply, SARSA Max estimates the value of the next state according to best known policy rather than estimating the value with a semi-stochastic policy.\n",
    "\n",
    "Makes you wonder why you had to implement SARSA in the first place...\n",
    "\n",
    "\"Can't be starting all these problems if you cannot solve them.\"\n",
    "\n",
    "\\-Cardi B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SARSA Max (Q-learning) Pseudocode\n",
    "---\n",
    "\n",
    "initialize epsilon\n",
    "\n",
    "for episode in num_episodes:\n",
    "\n",
    "    decay epsilon\n",
    "    state <- initial state\n",
    "    t <- 0\n",
    "    repeat until terminal state:\n",
    "        chose action from Q epsilon greedy policy\n",
    "        take action A_t and observe R_t+1, S_t+1\n",
    "        **chose action A_t+1 from max value in Q-table for S_t+1**\n",
    "        update Q-table\n",
    "        S_t <- S_t+1\n",
    "\n",
    "---\n",
    "\n",
    "Recall that the Q-table update for SARSA Max is:\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\max_{a \\in A}Q(S_{t+1}, a) - Q(S_t, A_t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Q-learning Algorithm\n",
    "    :param env: open ai gym environment\n",
    "    :param num_episodes: number of episodes to run\n",
    "    :param alpha: weight for the Q-tabel running average\n",
    "    :param gamma: discount factor for future rewards\n",
    "    :return: (dict) Q-table and (list) score for each episode\n",
    "    \"\"\"\n",
    "    # useful for epsilon greedy action selection...\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "\n",
    "    # initialize performance monitor\n",
    "    scores = []\n",
    "\n",
    "    # epsilon decay parameters\n",
    "    eps = 1\n",
    "    eps_decay = 0.9\n",
    "    eps_end = 0.000001\n",
    "\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # decay epsilon\n",
    "        eps = max(eps * eps_decay, eps_end)\n",
    "\n",
    "        ## TODO: complete remaining pseudocode\n",
    "        ## HINT: env.reset() returns the initial state\n",
    "        ## HINT: env.step() takes an int (action) and returns a tuple of (next_state, reward, done, info)\n",
    "\n",
    "    return Q, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PASSED** indicates that your Q-table is sufficiently close to the true value, well done!\n",
    "**PLEASE TRY AGAIN** indicates that you have a little more work to do.\n",
    "\n",
    "**IMPORTANT** For the tests to pass you can change num_episodes and alpha, but you must leave gamma in the default value.\n",
    "Once the tests pass, and your function is working correctly, then feel free to alter gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsamax, scores_sarsamax = q_learning(env, num_episodes=10000, alpha=0.01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsamax = np.array([np.argmax(Q_sarsamax[key]) if key in Q_sarsamax else -1 for key in np.arange(48)]).reshape((4,12))\n",
    "run_check('td_control_check', policy_sarsamax)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsamax)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_sarsamax[key]) if key in Q_sarsamax else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expected SARSA\n",
    "\n",
    "Lastly we will implement Expected SARSA.\n",
    "This will hearken back to our early days of sums over probabilities.\n",
    "\n",
    "Yay....\n",
    "\n",
    "As with SARSA Max we will choose the next action using the Q-table with an epsilon greedy policy.\n",
    "But when we update Q(S_t, A_t) we will use the **expected** value of next state action pair.\n",
    "This expected value utilizes our good old friend the product rule!\n",
    "\n",
    "Yay...\n",
    "\n",
    "Where the expected value for any action is the estimated value times the probability of choosing that action according to the policy.\n",
    "\n",
    "It is otherwise very similar to SARSA Max...\n",
    "\n",
    "\"You'll never get out of this maze!\"\n",
    "\n",
    "\\-Phish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Expected SARSA Pseudocode\n",
    "---\n",
    "\n",
    "initialize epsilon\n",
    "\n",
    "    decay epsilon\n",
    "    state <- initial state\n",
    "    t <- 0\n",
    "    repeat until terminal state:\n",
    "        chose action from Q with epsilon greedy policy\n",
    "        take action A_t and observe R_t+1, S_t+1\n",
    "        **chose action A_t+1 from max value in Q-table for S_t+1**\n",
    "        update Q-table\n",
    "        S_t <- S_t+1\n",
    "\n",
    "---\n",
    "\n",
    "Recall that the Q-table update for SARSA Max is:\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\sum_{a \\in A}\\pi(a \\vert S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t))$$\n",
    "\n",
    "$\\pi(a \\vert S_{t+1}) = \\Bigg\\{$\n",
    "$\\begin{array}{rcl}\n",
    "                            1 - \\epsilon + \\frac{\\epsilon}{\\text{num_actions}} & \\mbox{for} & \\text{arg}\\max_{a \\in A}Q(S_{t+1}, a) \\\\\n",
    "                            \\frac{\\epsilon}{\\text{num_actions}} & \\mbox{for} & \\neg{\\text{arg}\\max_{a \\in A}Q(S_{t+1}, a) }\n",
    "                            \\end{array}\n",
    "                            $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Expected SARSA Algorithm\n",
    "    :param env: open ai gym environment\n",
    "    :param num_episodes: number of episodes to run\n",
    "    :param alpha: weight for the Q-tabel running average\n",
    "    :param gamma: discount factor for future rewards\n",
    "    :return: (dict) Q-table and (list) score for each episode\n",
    "    \"\"\"\n",
    "    # useful for epsilon greedy action selection...\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "\n",
    "    # initialize performance monitor\n",
    "    scores = []\n",
    "\n",
    "    # epsilon decay parameters\n",
    "    eps = 1\n",
    "    eps_decay = 0.9\n",
    "    eps_end = 0.000001\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # decay epsilon\n",
    "        eps = max(eps * eps_decay, eps_end)\n",
    "\n",
    "        ## TODO: complete the remaining pseudocode\n",
    "        ## HINT: env.reset() returns the initial state\n",
    "        ## HINT: env.step() takes an int (action) and returns a tuple of (next_state, reward, done, info)\n",
    "\n",
    "    return Q, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PASSED** indicates that your Q-table is sufficiently close to the true value, well done!\n",
    "**PLEASE TRY AGAIN** indicates that you have a little more work to do.\n",
    "\n",
    "**IMPORTANT** For the tests to pass you can change num_episodes and alpha, but you must leave gamma in the default value.\n",
    "Once the tests pass, and your function is working correctly, then feel free to alter gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_expsarsa, scores_exp_sarsa = expected_sarsa(env, num_episodes=10000, alpha=1)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_expsarsa = np.array([np.argmax(Q_expsarsa[key]) if key in Q_expsarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "run_check('td_control_check', policy_expsarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_expsarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_expsarsa[key]) if key in Q_expsarsa else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Performance Comparison\n",
    "---\n",
    "Use the scores list from each algorithm to plot the 10 episode moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "avg_sarsa = np.convolve(np.array(scores_sarsa), np.ones(10), mode=\"valid\")\n",
    "avg_sarsamax = np.convolve(np.array(scores_sarsamax), np.ones(10), mode=\"valid\")\n",
    "avg_exp_sarsa = np.convolve(np.array(scores_exp_sarsa), np.ones(10), mode=\"valid\")\n",
    "plt.plot(np.arange(len(avg_sarsa)), avg_sarsa, label=\"SARSA\")\n",
    "plt.plot(np.arange(len(avg_sarsamax)), avg_sarsamax, label=\"Q-Learning\")\n",
    "plt.plot(np.arange(len(avg_exp_sarsa)), avg_exp_sarsa, label=\"Expected SARSA\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}